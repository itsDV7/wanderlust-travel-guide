#!/usr/bin/env python3
"""
Simple test for VQA model loading
"""

import os
os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'

def test_vqa_loading():
    """Test VQA model loading step by step"""
    print("Testing VQA model loading...")
    
    try:
        import torch
        print(f"âœ… PyTorch imported: {torch.__version__}")
    except Exception as e:
        print(f"âŒ PyTorch import failed: {e}")
        return False
    
    try:
        from transformers import AutoModel, AutoTokenizer
        print("âœ… Transformers imported")
    except Exception as e:
        print(f"âŒ Transformers import failed: {e}")
        return False
    
    # Test device selection
    try:
        if torch.cuda.is_available():
            device = 'cuda'
            print(f"âœ… CUDA available: {torch.cuda.get_device_name(0)}")
        elif torch.backends.mps.is_available():
            device = 'mps'
            print("âœ… MPS available")
        else:
            device = 'cpu'
            print("âœ… Using CPU")
        
        print(f"âœ… Device selected: {device}")
    except Exception as e:
        print(f"âŒ Device selection failed: {e}")
        return False
    
    # Test dtype selection
    try:
        if device == 'cuda':
            if torch.cuda.is_available() and torch.cuda.get_device_capability()[0] >= 8:
                dtype = torch.bfloat16
            else:
                dtype = torch.float16
        elif device == 'mps':
            dtype = torch.float16
        else:
            dtype = torch.float32
        
        print(f"âœ… Dtype selected: {dtype}")
    except Exception as e:
        print(f"âŒ Dtype selection failed: {e}")
        return False
    
    try:
        print("ğŸ”„ Loading model...")
        model = AutoModel.from_pretrained(
            'openbmb/MiniCPM-V-2', 
            trust_remote_code=True, 
            torch_dtype=dtype
        )
        print("âœ… Model loaded successfully")
    except Exception as e:
        print(f"âŒ Model loading failed: {e}")
        import traceback
        print(f"Full traceback: {traceback.format_exc()}")
        return False
    
    try:
        print(f"ğŸ”„ Moving model to {device}...")
        model = model.to(device=device, dtype=dtype)
        print(f"âœ… Model moved to {device}")
    except Exception as e:
        print(f"âŒ Model device move failed: {e}")
        print("ğŸ”„ Trying fallback to CPU...")
        try:
            device = 'cpu'
            dtype = torch.float32
            model = model.to(device=device, dtype=dtype)
            print("âœ… Model moved to CPU (fallback)")
        except Exception as fallback_error:
            print(f"âŒ CPU fallback also failed: {fallback_error}")
            return False
    
    try:
        print("ğŸ”„ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            'openbmb/MiniCPM-V-2', 
            trust_remote_code=True
        )
        print("âœ… Tokenizer loaded successfully")
    except Exception as e:
        print(f"âŒ Tokenizer loading failed: {e}")
        return False
    
    try:
        print("ğŸ”„ Setting model to eval mode...")
        model.eval()
        print("âœ… Model set to eval mode")
    except Exception as e:
        print(f"âŒ Model eval mode failed: {e}")
        return False
    
    print(f"ğŸ‰ All VQA components loaded successfully on {device}!")
    return True

if __name__ == "__main__":
    test_vqa_loading() 